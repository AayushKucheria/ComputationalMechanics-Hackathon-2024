## Exploring Hierarchical Structure Representation in Transformer Models through Computational Mechanics

Authors: Aayush Kucheria, Olli Järviniemi, Udayanto Dwi Atmojo, Konsta Tiilikainen

## Abstract

Interpretability, a key property of trustworthy AI, allows one to understand the internal workings of AI systems. Previous work in interpretability within the explainable AI body of knowledge hasn’t focused on a deep understanding of AI systems, leading to a “black-box” problem. Mechanistic interpretability attempts to address this gap, and there have also been recent approaches that utilize computational mechanics to more formally engage with the internal phenomena. However, this domain is still in its infancy, where, for example, the data generating processes considered, are relatively simple and it remains unknown whether the approaches can scale as the (data-generating) process gets more complex. 

This work attempts to explore how an approach based on computational mechanics can cope when a more complex hierarchical generative process is involved, i.e, a process that comprises Hidden Markov Models (HMMs) whose transition probabilities change over time. 

We find that small transformer models are capable of modeling such changes in an HMM. However, our preliminary investigations did not find geometrically represented probabilities for different hypotheses.

Future work could include performing more systematic exploration of the types of hierarchical processes studied in this work, and to more confidently test whether the representations generated by the computational mechanics framework are able to model more complex phenomena.

Keywords: Computational Mechanics, Transformers,  Interpretability, AI Alignment.

Full report [here](https://github.com/AayushKucheria/ComputationalMechanics-Hackathon-2024/blob/main/report.pdf).
